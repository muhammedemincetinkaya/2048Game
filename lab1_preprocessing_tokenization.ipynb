{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muhammedemincetinkaya/2048Game/blob/master/lab1_preprocessing_tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYc27BU4uCSG"
      },
      "outputs": [],
      "source": [
        "__author__ = \"Pranava Madhyastha\"\n",
        "__version__ = \"INM434/IN3045 City, University of London, Spring 2026\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenisation: a brief overview\n",
        "\n",
        "This is our first notebook for the module and we are going to focus on tokenisation. Please make sure to login to your respective google account to get started.\n",
        "\n",
        "A tokenizer is used to split an input string into separate tokens or pieces, where each token represents a meaningful element in the input string. This is one of the most important steps in NLP. This step is also called text-preprocessing.\n",
        "\n",
        "Tokenisation helps create a vocabulary of unique tokens overwhich we will run a variety of NLP algorithms. It mainly helps standardise the input data and help us squeeze in information from the long tail. It, in some cases, helps us to break the input text down into smaller, sometimes linguistically meaningful, units.\n",
        "\n",
        "In this notebook, we will first build a simple tokeniser which only splits on white space. We will then look at a toy morphological analyser. We will then build a simplified version of subword based tokeniser.\n",
        "\n",
        "---\n",
        "\n",
        "How does this notebook work:\n",
        "\n",
        "  * There will be some cells with \"TODO\": you will have to fill the code for the placeholder \"YOUR CODE HERE\".\n",
        "  * You will also notice \"ADVANCED TODO\", this is for students who are indeed capable of writing\n",
        "\n",
        "  * Each cell should take a few seconds to run, so if it is taking longer, there may be bug."
      ],
      "metadata": {
        "id": "1l01hmqxu_aR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple tokeniser\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "We will make use of the built in regular expression library in python to construct this tokeniser. Please refer to Chapter 2 (SLP: Jurafsky and Martin) references on regular expressions.\n",
        "\n",
        "The tokeniser below will take split input text into separate words based on word boundaries. This is defined using the regular expression pattern \"\\b\\w+\\b\". The re.findall() function is used to extract all the tokens from the text that match the pattern, and the list of tokens is returned.\n"
      ],
      "metadata": {
        "id": "RJQMAos2vaI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def tokenise(text):\n",
        "    # Define a regular expression pattern to split the text into tokens\n",
        "    pattern = r'\\b\\w+\\b'\n",
        "\n",
        "    # Use the re.findall() function to extract all the tokens from the text\n",
        "    tokens = re.findall(pattern, text)\n",
        "\n",
        "    # Return the list of tokens\n",
        "    return tokens\n",
        "\n",
        "# Sample input for the tokeniser\n",
        "text = \"This is IN 3045/INM 434 Natural Language Procssing. This is some sample text for tokeniser\"\n",
        "tokens = tokenise(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "WEsmbqQWvddu",
        "outputId": "394b8d3f-7e79-43e7-f223-c557813bd9cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'IN', '3045', 'INM', '434', 'Natural', 'Language', 'Procssing', 'This', 'is', 'some', 'sample', 'text', 'for', 'tokeniser']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: What does '\\b\\w+\\b' mean?"
      ],
      "metadata": {
        "id": "3TsA2hnFvhPO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Building a Sentiment Classifier <a name=\"classifier\"></a>\n",
        "\n",
        "Let's build a sentiment classifier that predicts whether a movie review is **positive** or **negative** -- this is one of the classic NLP tasks.\n",
        "\n",
        "## Our Plan is:\n",
        "\n",
        "1. **Tokenise**: Input the text by tokenising the text\n",
        "2. **Vectorise**: Convert the elements of the text into a bag-of-words representation\n",
        "3. **Train**: Learn which words indicate positive/negative sentiment\n",
        "4. **Predict**: Classify new reviews"
      ],
      "metadata": {
        "id": "h4rVnmIWvl40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Our training data: movie reviews with sentiment labels\n",
        "training_data = [\n",
        "    # Positive reviews\n",
        "    (\"This movie is amazing and wonderful\", \"positive\"),\n",
        "    (\"I loved every minute of this film\", \"positive\"),\n",
        "    (\"Brilliant acting and great story\", \"positive\"),\n",
        "    (\"One of the best movies I have seen\", \"positive\"),\n",
        "    (\"Absolutely fantastic and entertaining\", \"positive\"),\n",
        "    (\"A beautiful and moving experience\", \"positive\"),\n",
        "    (\"Highly recommend this masterpiece\", \"positive\"),\n",
        "    (\"Excellent film with superb performances\", \"positive\"),\n",
        "    (\"I really enjoyed this movie\", \"positive\"),\n",
        "    (\"What a great and fun movie\", \"positive\"),\n",
        "\n",
        "    # Negative reviews\n",
        "    (\"This movie is terrible and boring\", \"negative\"),\n",
        "    (\"I hated every minute of this film\", \"negative\"),\n",
        "    (\"Awful acting and stupid story\", \"negative\"),\n",
        "    (\"One of the worst movies ever made\", \"negative\"),\n",
        "    (\"Absolutely dreadful waste of time\", \"negative\"),\n",
        "    (\"A painful and tedious experience\", \"negative\"),\n",
        "    (\"Do not waste your money on this\", \"negative\"),\n",
        "    (\"Horrible film with bad performances\", \"negative\"),\n",
        "    (\"I really disliked this movie\", \"negative\"),\n",
        "]\n",
        "total = len(training_data)\n",
        "\n",
        "positive = sum(1 for text, label in training_data if label == \"positive\")\n",
        "negative = sum(1 for text, label in training_data if label == \"negative\")\n",
        "\n",
        "print(f\"Total examples: {total}\")\n",
        "print(f\"Positive examples: {positive}\")\n",
        "print(f\"Negative examples: {negative}\")"
      ],
      "metadata": {
        "id": "g4KvIFL2vjrr",
        "outputId": "e66b597e-858d-4612-d3b0-62a6a6506299",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total examples: 19\n",
            "Positive examples: 10\n",
            "Negative examples: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Please write code to list the number of training examples in total, with the number of positive and negative examples."
      ],
      "metadata": {
        "id": "ommNHxaMwNsu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Tokenizer\n",
        "\n",
        "# First, we need to split text into words. Let's start with a simple approach:"
      ],
      "metadata": {
        "id": "R9XNg_ldwHaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_tokenize(text):\n",
        "\n",
        "    # TODO - write code to convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Write code to Split on word boundaries, keep only alphanumeric\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "QbXs63CKwEm-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Vocabulary\n",
        "\n",
        "We need to know all unique words in our training data -- this is the VOCABULARY of our model"
      ],
      "metadata": {
        "id": "ryww5nJhwyOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocabulary(data, tokenize_fn):\n",
        "\n",
        "    # we will use COUNTER class from python -- https://docs.python.org/3/library/collections.html\n",
        "    word_counts = Counter()\n",
        "\n",
        "    for text, label in data:\n",
        "        tokens = tokenize_fn(text)\n",
        "        word_counts.update(tokens)\n",
        "\n",
        "    # TODO: What is counter doing?\n",
        "\n",
        "    # Create word-to-index mapping\n",
        "    vocab = {word: idx for idx, word in enumerate(sorted(word_counts.keys()))}\n",
        "\n",
        "    return vocab, word_counts\n",
        "\n",
        "# Build vocabulary\n",
        "vocab, word_counts = build_vocabulary(training_data, simple_tokenize)\n",
        "\n",
        "# Print vocabulary size\n",
        "print(\"Vocabulary size:\", len(vocab))\n",
        "\n",
        "# List words from most frequent to least frequent\n",
        "print(\"\\nWords from most to least frequent:\")\n",
        "for word, count in word_counts.most_common():\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "id": "0gY9bC7Mwutt",
        "outputId": "a82dd616-224d-4e5d-b647-8c05cd436b63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 61\n",
            "\n",
            "Words from most to least frequent:\n",
            "this: 8\n",
            "and: 8\n",
            "movie: 5\n",
            "i: 5\n",
            "of: 5\n",
            "film: 4\n",
            "a: 3\n",
            "is: 2\n",
            "every: 2\n",
            "minute: 2\n",
            "acting: 2\n",
            "great: 2\n",
            "story: 2\n",
            "one: 2\n",
            "the: 2\n",
            "movies: 2\n",
            "absolutely: 2\n",
            "experience: 2\n",
            "with: 2\n",
            "performances: 2\n",
            "really: 2\n",
            "waste: 2\n",
            "amazing: 1\n",
            "wonderful: 1\n",
            "loved: 1\n",
            "brilliant: 1\n",
            "best: 1\n",
            "have: 1\n",
            "seen: 1\n",
            "fantastic: 1\n",
            "entertaining: 1\n",
            "beautiful: 1\n",
            "moving: 1\n",
            "highly: 1\n",
            "recommend: 1\n",
            "masterpiece: 1\n",
            "excellent: 1\n",
            "superb: 1\n",
            "enjoyed: 1\n",
            "what: 1\n",
            "fun: 1\n",
            "terrible: 1\n",
            "boring: 1\n",
            "hated: 1\n",
            "awful: 1\n",
            "stupid: 1\n",
            "worst: 1\n",
            "ever: 1\n",
            "made: 1\n",
            "dreadful: 1\n",
            "time: 1\n",
            "painful: 1\n",
            "tedious: 1\n",
            "do: 1\n",
            "not: 1\n",
            "your: 1\n",
            "money: 1\n",
            "on: 1\n",
            "horrible: 1\n",
            "bad: 1\n",
            "disliked: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Counter keeps track of how many times each token appears"
      ],
      "metadata": {
        "id": "ZB4r-HZOih5L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Write code to print the size of the vocabulary and also list the most frequent to least frequent words."
      ],
      "metadata": {
        "id": "na1A7j49xjmY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag-of-Words Representation\n",
        "\n",
        "Convert each document to a vector (in our case an array) of word counts"
      ],
      "metadata": {
        "id": "kykeFFGSx5_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_bow(text, vocab, tokenize_fn):\n",
        "    # Initialize vector of zeros\n",
        "    bow = np.zeros(len(vocab))\n",
        "\n",
        "    # Count words\n",
        "    tokens = tokenize_fn(text)\n",
        "    for token in tokens:\n",
        "        if token in vocab:\n",
        "            bow[vocab[token]] += 1\n",
        "\n",
        "    return bow\n",
        "\n",
        "\n",
        "example  = \"This movie is \"\n",
        "bow_vector = text_to_bow(example, vocab, simple_tokenize)\n",
        "\n",
        "# Print full bag-of-words vector\n",
        "print(\"BoW vector:\")\n",
        "print(bow_vector)\n",
        "\n",
        "# Print tokens with non-zero entries\n",
        "print(\"\\nNon-zero tokens in BoW:\")\n",
        "for word, idx in vocab.items():\n",
        "    if bow_vector[idx] > 0:\n",
        "        print(f\"{word}: {bow_vector[idx]}\")"
      ],
      "metadata": {
        "id": "lzBrPl_ExpZN",
        "outputId": "d00a2daf-9ad9-4da0-be89-4bc76b793781",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoW vector:\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "\n",
            "Non-zero tokens in BoW:\n",
            "is: 1.0\n",
            "movie: 1.0\n",
            "this: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Print out the bag of word vector. Also, print out all the \"tokens\" with non-zero entries."
      ],
      "metadata": {
        "id": "JGY-Fw2lybpS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now Let us train a Simple Classifier\n",
        "\n",
        "We will now use a very simple classifier -- the Naive Bayes classifier which is a simple but effective classifier for text.\n",
        "\n",
        "The idea: estimate P(word | positive) and P(word | negative), then use Bayes' rule.\n",
        "\n",
        "Essentially we are seeing If a review is positive, will it have the word 'good'?. Then, we ask, if I indeed see the word 'good', what is the probability the review is positive?. It allows us to update our belief about the review's sentiment as we read each word -- this is a fairly naive way of approaching language -- here we don't care about the rich structure!"
      ],
      "metadata": {
        "id": "Dhj1XdiCzQgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NaiveBayesClassifier:\n",
        "\n",
        "\n",
        "    def __init__(self, tokenize_fn):\n",
        "        self.tokenize_fn = tokenize_fn\n",
        "        self.vocab = None\n",
        "        self.word_probs = {}  # P(word | class)\n",
        "        self.class_probs = {}  # P(class)\n",
        "\n",
        "    def train(self, data):\n",
        "\n",
        "        # Build vocabulary\n",
        "        self.vocab, word_counts = build_vocabulary(data, self.tokenize_fn)\n",
        "\n",
        "        # Count words per class -- TODO: Why do we need this?\n",
        "        class_word_counts = defaultdict(Counter)\n",
        "        class_doc_counts = Counter()\n",
        "\n",
        "        for text, label in data:\n",
        "            tokens = self.tokenize_fn(text)\n",
        "            class_word_counts[label].update(tokens)\n",
        "            class_doc_counts[label] += 1\n",
        "\n",
        "        # Calculate probabilities with smoothing -- this is done to prevent division by zero!\n",
        "        total_docs = len(data)\n",
        "        vocab_size = len(self.vocab)\n",
        "\n",
        "        for label in class_doc_counts:\n",
        "            # P(class)\n",
        "            self.class_probs[label] = class_doc_counts[label] / total_docs\n",
        "\n",
        "            # P(word | class) with Laplace smoothing\n",
        "            total_words = sum(class_word_counts[label].values())\n",
        "            self.word_probs[label] = {}\n",
        "\n",
        "            for word in self.vocab:\n",
        "                count = class_word_counts[label][word]\n",
        "                # Laplace smoothing -- add 1 to avoid zero probabilities\n",
        "                self.word_probs[label][word] = (count + 1) / (total_words + vocab_size)\n",
        "\n",
        "        print(f\"Trained on {total_docs} documents\")\n",
        "        print(f\"Vocabulary size is: {vocab_size}\")\n",
        "        print(f\"Total number of classes: {list(class_doc_counts.keys())}\")\n",
        "\n",
        "    def predict(self, text, return_probs=False):\n",
        "\n",
        "        tokens = self.tokenize_fn(text)\n",
        "\n",
        "        # Calculate log probability for each class\n",
        "        log_probs = {}\n",
        "\n",
        "        for label in self.class_probs:\n",
        "            # Start with log P(class)\n",
        "            log_prob = np.log(self.class_probs[label])\n",
        "\n",
        "            # Add log P(word | class) for each word\n",
        "            for token in tokens:\n",
        "                if token in self.word_probs[label]:\n",
        "                    log_prob += np.log(self.word_probs[label][token])\n",
        "                # Ignore unknown words (or could use a small probability)\n",
        "\n",
        "            log_probs[label] = log_prob\n",
        "\n",
        "        # Predict class with highest probability\n",
        "        predicted = max(log_probs, key=log_probs.get)\n",
        "\n",
        "        if return_probs:\n",
        "            # Convert to actual probabilities\n",
        "            max_log = max(log_probs.values())\n",
        "            probs = {k: np.exp(v - max_log) for k, v in log_probs.items()}\n",
        "            total = sum(probs.values())\n",
        "            probs = {k: v/total for k, v in probs.items()}\n",
        "            return predicted, probs\n",
        "\n",
        "        return predicted\n",
        "\n",
        "    def get_important_words(self, n=10):\n",
        "\n",
        "        important = {}\n",
        "\n",
        "        for label in self.class_probs:\n",
        "            other_label = [l for l in self.class_probs if l != label][0]\n",
        "\n",
        "            ratios = []\n",
        "            for word in self.vocab:\n",
        "                ratio = self.word_probs[label][word] / self.word_probs[other_label][word]\n",
        "                ratios.append((word, ratio))\n",
        "\n",
        "            ratios.sort(key=lambda x: x[1], reverse=True)\n",
        "            important[label] = ratios[:n]\n",
        "\n",
        "        return important\n",
        "\n",
        "# Let us now train our classifier\n",
        "classifier = NaiveBayesClassifier(simple_tokenize)\n",
        "classifier.train(training_data)"
      ],
      "metadata": {
        "id": "2xulYqYn1Nd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## So what has it learned?\n",
        "\n",
        "Let us see which words are most indicative of positive vs negative sentiment?"
      ],
      "metadata": {
        "id": "dImRnZe13jAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "important_words = classifier.get_important_words(10)\n"
      ],
      "metadata": {
        "id": "qlq2e2cs33v8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Print out the most important words and their corresponding ratios"
      ],
      "metadata": {
        "id": "UMbH-Ki6355k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Does it work? <a name=\"test\"></a>\n",
        "\n",
        "Let's test our classifier on some examples:"
      ],
      "metadata": {
        "id": "pUKlvcef4Xxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_examples = [\n",
        "    \"This movie is amazing\",\n",
        "    \"I loved this film\",\n",
        "    \"Terrible and boring\",\n",
        "    \"The worst movie ever\",\n",
        "    \"A great and entertaining film\",\n",
        "    \"Awful waste of time\",\n",
        "]\n",
        "\n",
        "print(\"Testing our classifier:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for text in test_examples:\n",
        "    pred, probs = classifier.predict(text, return_probs=True)\n",
        "    confidence = probs[pred] * 100\n",
        "    emoji = \"correct\" if pred == \"positive\" else \"incorrect\"\n",
        "    print(f\"{emoji} '{text}'\")\n",
        "    print(f\"   --> {pred} ({confidence:.1f}% confident)\\n\")"
      ],
      "metadata": {
        "id": "ppDmvToP4VjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How did it do?\n",
        "\n",
        "Our simple bag-of-words classifier correctly identifies sentiment for straightforward cases.\n",
        "\n",
        "Even this simple approach works because:\n",
        "- Positive reviews tend to use words like \"amazing\", \"loved\", \"great\"\n",
        "- Negative reviews tend to use words like \"terrible\", \"awful\", \"worst\"\n",
        "\n",
        "Now let's look at some tricky cases!"
      ],
      "metadata": {
        "id": "kMxT9h515a7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tricky_examples = [\n",
        "    # Negation ones\n",
        "    (\"This movie is not good\", \"negative\"),\n",
        "    (\"I don't like this film\", \"negative\"),\n",
        "    (\"Not bad at all\", \"positive\"),\n",
        "    (\"I didn't hate it\", \"positive\"),\n",
        "\n",
        "    # Say there is some sarcasm or contrasting bits\n",
        "    (\"Oh great, another terrible movie\", \"negative\"),\n",
        "    (\"The acting was good but the story was awful\", \"mixed\"),\n",
        "\n",
        "    # Say we have words not seen in the data ?\n",
        "    (\"This film is phenomenal\", \"positive\"),\n",
        "    (\"Utterly abysmal\", \"negative\"),\n",
        "    (\"A cinematic triumph\", \"positive\"),\n",
        "\n",
        "    # Intensity bits\n",
        "    (\"This movie is good\", \"positive\"),\n",
        "    (\"This movie is REALLY good\", \"positive\"),\n",
        "    (\"This movie is soooo good!!!\", \"positive\"),\n",
        "\n",
        "    # Word variations\n",
        "    (\"I loved it\", \"positive\"),\n",
        "    (\"I'm loving it\", \"positive\"),\n",
        "    (\"Lovable characters\", \"positive\"),\n",
        "]\n",
        "\n",
        "print(\"Testing on TRICKY examples:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "correct = 0\n",
        "wrong = 0\n",
        "\n",
        "for text, expected in tricky_examples:\n",
        "    pred, probs = classifier.predict(text, return_probs=True)\n",
        "    confidence = probs[pred] * 100\n",
        "\n",
        "    if expected == \"mixed\":\n",
        "        status = \"dont know\"  # Can't really be right or wrong\n",
        "    elif pred == expected:\n",
        "        status = \"correct\"\n",
        "        correct += 1\n",
        "    else:\n",
        "        status = \"incorrect\"\n",
        "        wrong += 1\n",
        "\n",
        "    print(f\"{status} '{text}'\")\n",
        "    print(f\"   Expected: {expected}, Got: {pred} ({confidence:.1f}%)\\n\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(f\"Results: {correct} correct, {wrong} wrong (excluding 'mixed')\")"
      ],
      "metadata": {
        "id": "OBlUzGp65ibo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced TODO:\n",
        "Can we come up with a negation aware tokenizer? That is -- say you tokenize such that \"the movie is not good\" is tokenized as [\"the\", \"movie\", \"is\", \"not\",  \"NOT_good\"]."
      ],
      "metadata": {
        "id": "uNBxGhT36D17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# 1. Create a new classifier with tokenize_with_negation\n",
        "# 2. Train it on training_data\n",
        "# 3. Test on the negation examples from tricky_examples\n",
        "\n",
        "classifier_negation = NaiveBayesClassifier(tokenize_with_negation)\n",
        "classifier_negation.train(training_data)\n",
        "\n",
        "# Test on negation examples\n",
        "negation_tests = [\n",
        "    (\"This movie is not good\", \"negative\"),\n",
        "    (\"I don't like this film\", \"negative\"),\n",
        "    (\"Not bad at all\", \"positive\"),\n",
        "]\n",
        "\n",
        "print(\"Testing negation-aware tokenizer:\")\n",
        "for text, expected in negation_tests:\n",
        "    tokens = tokenize_with_negation(text)\n",
        "    pred = classifier_negation.predict(text)\n",
        "    status = \"correct\" if pred == expected else \"incorrect\"\n",
        "    print(f\"{status} '{text}'\")\n",
        "    print(f\"   Tokens: {tokens}\")\n",
        "    print(f\"   Expected: {expected}, Got: {pred}\\n\")"
      ],
      "metadata": {
        "id": "5jDHOlUG7fZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Better Tokenization with BPE <a name=\"part5\"></a>\n",
        "\n",
        "**Byte-Pair Encoding (BPE)** learns a vocabulary from data that balances:\n",
        "- Small vocabulary size\n",
        "- Meaningful units (not just characters)\n",
        "- No OOV problem (can always fall back to characters)\n",
        "\n",
        "This is what GPT, BERT, and most modern LLMs use!\n",
        "\n",
        "\n",
        "Sennrich, Rico, Barry Haddow, and Alexandra Birch. \"Neural Machine Translation of Rare Words with Subword Units.\" Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Vol. 1. 2016. http://www.aclweb.org/anthology/P16-1162\n",
        "\n",
        "Main source: https://github.com/rsennrich/subword-nmt\n",
        "\n",
        "Below is a simple version of the algorithm.\n",
        "\n",
        "Two stages: token learner and token segmenter.\n",
        "\n",
        "Let us first look at token learner, this involves:\n",
        "\n",
        "*  computing the frequencies of all words in a corpus (we do it synthetically here)\n",
        "*  start with characters as the basic vocab (characters seen in the corpus)\n",
        "* to obtain vocabulary of n-merge operations:\n",
        "    - Obtain most frequent pairs of characters in the corpus\n",
        "    - add the pair to the list of merges\n",
        "    - add merged characters to the vocab\n",
        "* iterate n times\n",
        "\n",
        "The code below performs this operation."
      ],
      "metadata": {
        "id": "m77JSqgF8A3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "def compute_pair_frequencies(vocab):\n",
        "\n",
        "    pairs = collections.defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols) - 1):\n",
        "            pairs[symbols[i], symbols[i + 1]] += freq\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def merge_vocab(pair, vocab):\n",
        "\n",
        "    new_vocab = {}\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    pattern = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "\n",
        "    for word in vocab:\n",
        "        new_word = pattern.sub(''.join(pair), word)\n",
        "        new_vocab[new_word] = vocab[word]\n",
        "\n",
        "    return new_vocab\n",
        "\n",
        "\n",
        "def train_bpe(corpus_words, num_merges):\n",
        "\n",
        "    # Initialize: split each word into characters + end marker\n",
        "    vocab = {}\n",
        "    for word, freq in corpus_words.items():\n",
        "        symbols = ' '.join(list(word)) + ' </w>'\n",
        "        vocab[symbols] = freq\n",
        "\n",
        "    bpe_codes = {}\n",
        "\n",
        "    print(\"BPE Training\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Initial vocabulary: {vocab}\\n\")\n",
        "\n",
        "    for i in range(num_merges):\n",
        "        pairs = compute_pair_frequencies(vocab)\n",
        "\n",
        "        if not pairs:\n",
        "            print(\"No more pairs to merge!\")\n",
        "            break\n",
        "\n",
        "        # Find most frequent pair\n",
        "        best_pair = max(pairs, key=pairs.get)\n",
        "        best_freq = pairs[best_pair]\n",
        "\n",
        "        # Merge these!\n",
        "        vocab = merge_vocab(best_pair, vocab)\n",
        "        bpe_codes[best_pair] = i\n",
        "\n",
        "        print(f\"Merge {i+1}: '{best_pair[0]}' + '{best_pair[1]}' -> '{best_pair[0]+best_pair[1]}' (freq: {best_freq})\")\n",
        "\n",
        "    print(f\"\\nFinal vocabulary: {vocab}\")\n",
        "\n",
        "    return bpe_codes, vocab"
      ],
      "metadata": {
        "id": "WJ7UFSi98Hy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to train it -- that is, it learns from data!"
      ],
      "metadata": {
        "id": "woqmYoop8la9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train BPE on a small corpus\n",
        "corpus_words = {\n",
        "    'low': 5,\n",
        "    'lower': 2,\n",
        "    'newest': 6,\n",
        "    'widest': 3,\n",
        "    'new': 4,\n",
        "}\n",
        "\n",
        "bpe_codes, final_vocab = train_bpe(corpus_words, num_merges=10)"
      ],
      "metadata": {
        "id": "lwzF9SX98iqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bpe_encode(word, bpe_codes):\n",
        "\n",
        "    # Start with characters + end marker\n",
        "    word = tuple(word) + ('</w>',)\n",
        "\n",
        "    while len(word) > 1:\n",
        "        # Find all adjacent pairs\n",
        "        pairs = [(word[i], word[i+1]) for i in range(len(word)-1)]\n",
        "\n",
        "        # Find pair with lowest merge rank (earliest learned)\n",
        "        min_pair = None\n",
        "        min_rank = float('inf')\n",
        "\n",
        "        for pair in pairs:\n",
        "            if pair in bpe_codes and bpe_codes[pair] < min_rank:\n",
        "                min_pair = pair\n",
        "                min_rank = bpe_codes[pair]\n",
        "\n",
        "        # If no pair can be merged, we're done\n",
        "        if min_pair is None:\n",
        "            break\n",
        "\n",
        "        # Merge all occurrences of min_pair\n",
        "        first, second = min_pair\n",
        "        new_word = []\n",
        "        i = 0\n",
        "\n",
        "        while i < len(word):\n",
        "            if i < len(word) - 1 and word[i] == first and word[i+1] == second:\n",
        "                new_word.append(first + second)\n",
        "                i += 2\n",
        "            else:\n",
        "                new_word.append(word[i])\n",
        "                i += 1\n",
        "\n",
        "        word = tuple(new_word)\n",
        "\n",
        "    # Clean up end marker for display\n",
        "    result = []\n",
        "    for token in word:\n",
        "        if token == '</w>':\n",
        "            continue\n",
        "        elif token.endswith('</w>'):\n",
        "            result.append(token[:-4])\n",
        "        else:\n",
        "            result.append(token)\n",
        "\n",
        "    return tuple(result)\n",
        "\n",
        "\n",
        "# Test BPE encoding\n",
        "test_words = ['low', 'lower', 'lowest', 'new', 'newer', 'newest', 'widest', 'unknown']\n",
        "\n",
        "print(\"\\nBPE Encoding Results:\")\n",
        "print(\"=\" * 40)\n",
        "for word in test_words:\n",
        "    encoded = bpe_encode(word, bpe_codes)\n",
        "    print(f\"{word:<12} -> {encoded}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WD1_fAwV8wFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, observe something interesting (even in this small set of examples). The BPE tokenizer has learned to split the words into:\n",
        "- `est` (superlative suffix)\n",
        "- `er` (comparative suffix, eventually)\n",
        "- `low`, `new` (common roots)\n",
        "\n",
        "This happened automatically from frequency, not from any linguistic rules!\n",
        "\n",
        "But, this may not be always true, as you see in the outputs!  "
      ],
      "metadata": {
        "id": "wVD8DAPn88zE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ADVANCED TODO: Train BPE on the full corpus above."
      ],
      "metadata": {
        "id": "S0kp6t1x9O-4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z9KoiaT_8z3x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}